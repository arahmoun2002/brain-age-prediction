{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 832) (1212, 1) (776, 832)\n"
     ]
    }
   ],
   "source": [
    "# Import the train and test data\n",
    "df_X_train = pd.read_csv('X_train.csv').iloc[:, 1:]\n",
    "df_y_train = pd.read_csv('y_train.csv').iloc[:, 1:]\n",
    "df_X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "X_train = df_X_train.to_numpy()\n",
    "y_train = df_y_train.to_numpy()\n",
    "X_test = df_X_test.to_numpy()\n",
    "id_test = X_test[:, 0]\n",
    "X_test = X_test[:, 1:]\n",
    "\n",
    "print(X_train.shape, y_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "(1212, 832) (1212, 1) (776, 832)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imput missing values using the mean of each column (basic : try to find more pertinent)\n",
    "\n",
    "# imput missing values using the k-neighbors imputer (more advanced)\n",
    "from sklearn.impute import KNNImputer   \n",
    "\n",
    "# Create the imputer object, with 50 neighbors\n",
    "imputer = KNNImputer(n_neighbors=10, weights='distance')\n",
    "\n",
    "# Fit the imputer object on the train data\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# Impute the missing values on the train and test data\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Check that there is no more missing values    \n",
    "print(np.isnan(X_train).sum(), np.isnan(X_test).sum())\n",
    "print(X_train.shape, y_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 666) (1212, 1) (776, 666)\n"
     ]
    }
   ],
   "source": [
    "# Remove features with low variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.1)  # remove features with more than 80% variance\n",
    "X_train = sel.fit_transform(X_train)\n",
    "X_test = sel.transform(X_test)\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop highly correlated features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming that X_train is your ndarray and it only contains feature columns\n",
    "df = pd.DataFrame(X_train)\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a set to hold the correlated columns\n",
    "corr_columns = set()\n",
    "\n",
    "# Iterate over the correlation matrix\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        # If the correlation between the columns is high, add it to the set\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            corr_columns.add(colname)\n",
    "\n",
    "# Get the indices of the relevant features\n",
    "relevant_features = [df.columns.get_loc(c) for c in df.columns if c not in corr_columns]\n",
    "\n",
    "X_train = X_train[:, relevant_features]\n",
    "X_test = X_test[:, relevant_features]\n",
    "# Print the relevant feature indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamrahmoun/miniconda3/envs/advancedmachinelearning/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import chi2\n",
    "# Select the most relevant features\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Create the SelectKBest with the mutual info strategy\n",
    "selector = SelectKBest(f_regression, k=100)\n",
    "\n",
    "# Fit the object to the training data\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 100) (1125, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Define the model\n",
    "clf = IsolationForest(max_samples=100, random_state=42, contamination='auto')\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(X_train)\n",
    "\n",
    "# Predict the anomalies in the data\n",
    "outliers = clf.predict(X_train)\n",
    "\n",
    "# Find the location of anomalies\n",
    "outlier_index = np.where(outliers == -1)\n",
    "\n",
    "# Remove outliers from X_train\n",
    "X_train = np.delete(X_train, outlier_index, axis=0)\n",
    "\n",
    "# Remove corresponding outliers from y_train\n",
    "y_train = np.delete(y_train, outlier_index, axis=0)\n",
    "\n",
    "# Print the shapes of the updated X_train and y_train\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 100) (900, 1) (225, 100) (225, 1) (776, 100)\n"
     ]
    }
   ],
   "source": [
    "#Split the data into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l_/j2hzwk0556nczvd_1c98dl1h0000gn/T/ipykernel_72243/712595561.py:12: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9288495188487036, 0.49874162367804675)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the ElasticNet model\n",
    "# model = ElasticNet(alpha=5, l1_ratio=1, random_state=0)\n",
    "# model = DecisionTreeRegressor(max_depth=3, min_samples_split=2, min_samples_leaf=1, random_state=0)\n",
    "model = RandomForestRegressor(max_depth=10, min_samples_split=2, min_samples_leaf=1, n_estimators=500, n_jobs=-1, random_state=42)\n",
    "# Fit the model to the training data\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "pred_test = model.predict(X_val)\n",
    "training_test = model.predict(X_train)\n",
    "\n",
    "train_sc = r2_score(y_train, training_test)\n",
    "val_sc = r2_score(y_test, pred_test)\n",
    "train_sc, val_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the test data and output it to a file \"out.csv\"\n",
    "y_out = model.predict(X_test)\n",
    "output = np.stack((id_test, y_out.flatten()), axis=-1)\n",
    "df_out = pd.DataFrame(output, columns=[\"id\", \"y\"])\n",
    "\n",
    "df_out.to_csv(\"out.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advancedmachinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
